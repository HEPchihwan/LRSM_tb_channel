{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d92790c",
   "metadata": {},
   "source": [
    "## Top tag : 0.9 , 120 250 \n",
    "## B tag : particle net tight 0.6734\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a145d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f79233fd",
   "metadata": {},
   "source": [
    "## BKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f985da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SingleMuon_C: incomplete data\n",
      "Skipping SingleMuon_B: incomplete data\n",
      "Skipping JetMET_C: incomplete data\n",
      "Skipping EGamma_D: incomplete data\n",
      "Skipping JetMET_D: incomplete data\n",
      "Skipping EGamma_B: incomplete data\n",
      "Skipping Tau_B: incomplete data\n",
      "Skipping Jpsito2Mu: incomplete data\n",
      "Skipping Tau_C: incomplete data\n",
      "Skipping Muon_C: incomplete data\n",
      "Skipping Tau_D: incomplete data\n",
      "Skipping EGamma_C: incomplete data\n",
      "Skipping Muon_D: incomplete data\n",
      "Skipping JetMET_B: incomplete data\n",
      "Wrote 47 entries (including total row) to summary.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# 1) 로그 파일이 들어있는 폴더\n",
    "LOG_DIR = \"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg/condorfile/out\"\n",
    "\n",
    "# 2) 결과를 담을 리스트\n",
    "rows = []\n",
    "\n",
    "# 3) 각 로그 파일 순회\n",
    "for logpath in glob.glob(os.path.join(LOG_DIR, \"*.log\")):\n",
    "    name = os.path.splitext(os.path.basename(logpath))[0]  # 파일명 베이스\n",
    "    with open(logpath, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 4) 정규표현식으로 숫자 추출\n",
    "    m_pass = re.search(r\"Passed events\\s*:\\s*([0-9]+)\", text)\n",
    "    m_weight = re.search(r\"Per-event weight\\s*:\\s*([0-9.eE\\+\\-]+)\", text)\n",
    "    m_exp = re.search(r\"Expected yield\\s*:\\s*([0-9.eE\\+\\-]+)\", text)\n",
    "\n",
    "    # 5) 모두 찾았을 때만 추가\n",
    "    if m_pass and m_weight and m_exp:\n",
    "        passed      = int(m_pass.group(1))\n",
    "        weight      = float(m_weight.group(1))\n",
    "        expected    = float(m_exp.group(1))\n",
    "        rows.append({\n",
    "            \"sample\": name,\n",
    "            \"Passed events\": passed,\n",
    "            \"Per-event weight\": weight,\n",
    "            \"Expected yield\": expected\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Skipping {name}: incomplete data\")\n",
    "\n",
    "# 6) Expected yield 기준으로 내림차순 정렬\n",
    "rows.sort(key=lambda x: x[\"Expected yield\"], reverse=True)\n",
    "\n",
    "# → 여기서 각 컬럼 합계 계산\n",
    "total_passed   = sum(r[\"Passed events\"]      for r in rows)\n",
    "total_weight   = sum(r[\"Per-event weight\"]   for r in rows)\n",
    "total_expected = sum(r[\"Expected yield\"]      for r in rows)\n",
    "\n",
    "# → 합계 행을 추가\n",
    "rows.append({\n",
    "    \"sample\":          \"Total\",\n",
    "    \"Passed events\":   total_passed,\n",
    "    \"Per-event weight\": total_weight,\n",
    "    \"Expected yield\":  total_expected\n",
    "})\n",
    "\n",
    "# 7) CSV로 저장 (Total 행 포함)\n",
    "out_csv = \"summary.csv\"\n",
    "with open(out_csv, 'w', newline='') as csvfile:\n",
    "    fieldnames = [\"sample\", \"Passed events\", \"Per-event weight\", \"Expected yield\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Wrote {len(rows)} entries (including total row) to {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6cfebb",
   "metadata": {},
   "source": [
    "## SIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99865d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8 entries to sig_summary.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# 0) background summary.csv 에서 B 값 읽어오기\n",
    "BG_SUMMARY = \"summary.csv\"  # 필요에 따라 경로를 조정하세요\n",
    "with open(BG_SUMMARY, newline='') as f_bg:\n",
    "    reader_bg = csv.DictReader(f_bg)\n",
    "    bg_rows = list(reader_bg)\n",
    "    # 마지막 행의 \"Expected yield\" 를 B 로 설정\n",
    "    B = float(bg_rows[-1][\"Expected yield\"])\n",
    "\n",
    "# 1) 로그 파일이 들어있는 폴더\n",
    "LOG_DIR = \"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig/condorfile/out\"\n",
    "\n",
    "rows = []\n",
    "for logpath in glob.glob(os.path.join(LOG_DIR, \"*.log\")):\n",
    "    name = os.path.splitext(os.path.basename(logpath))[0]\n",
    "    with open(logpath, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    m_pass   = re.search(r\"Passed events\\s*:\\s*([0-9]+)\", text)\n",
    "    m_weight = re.search(r\"Per-event weight\\s*:\\s*([0-9.eE\\+\\-]+)\", text)\n",
    "    m_exp    = re.search(r\"Expected yield\\s*:\\s*([0-9.eE\\+\\-]+)\", text)\n",
    "\n",
    "    if m_pass and m_weight and m_exp:\n",
    "        passed   = int(m_pass.group(1))\n",
    "        weight   = float(m_weight.group(1))\n",
    "        expected = float(m_exp.group(1))\n",
    "\n",
    "        # Z = S / sqrt(B)\n",
    "        Z = expected / math.sqrt(B) if B > 0 else float('nan')\n",
    "        # p-value = 1 - Phi(Z) = 0.5*erfc(Z/sqrt(2))\n",
    "        p_value = 0.5 * math.erfc(Z / math.sqrt(2))\n",
    "\n",
    "        rows.append({\n",
    "            \"sample\":           name,\n",
    "            \"Passed events\":    passed,\n",
    "            \"Per-event weight\": weight,\n",
    "            \"Expected yield\":   expected,\n",
    "            \"S_over_sqrtB\":     Z,\n",
    "            \"p_value\":          p_value\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Skipping {name}: incomplete data\")\n",
    "\n",
    "# 정렬 (선택)\n",
    "rows.sort(key=lambda x: x[\"Expected yield\"], reverse=True)\n",
    "\n",
    "# 7) CSV로 저장 (새 컬럼 포함)\n",
    "out_csv = \"sig_summary.csv\"\n",
    "with open(out_csv, 'w', newline='') as csvfile:\n",
    "    fieldnames = [\n",
    "        \"sample\",\n",
    "        \"Passed events\",\n",
    "        \"Per-event weight\",\n",
    "        \"Expected yield\",\n",
    "        \"S_over_sqrtB\",\n",
    "        \"p_value\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Wrote {len(rows)} entries to {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc1c89",
   "metadata": {},
   "source": [
    "## Bin by bin collector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8800bb5",
   "metadata": {},
   "source": [
    "#### bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608b6f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wrmass] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_wrmass_counts_pivot.csv (46 samples × 10 bins)\n",
      "[mll] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_mll_counts_pivot.csv (46 samples × 13 bins)\n",
      "[ptmu1] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_ptmu1_counts_pivot.csv (46 samples × 12 bins)\n",
      "[ptmu2] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_ptmu2_counts_pivot.csv (46 samples × 12 bins)\n",
      "[ptt] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_ptt_counts_pivot.csv (46 samples × 14 bins)\n",
      "[ptb] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_ptb_counts_pivot.csv (46 samples × 14 bins)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1) 베이스 디렉토리 설정\n",
    "base_dir = \"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg/condorfile\"\n",
    "\n",
    "# 2) 처리할 카운트 파일 종류 리스트\n",
    "file_types = [\"wrmass\", \"mll\", \"ptmu1\", \"ptmu2\", \"ptt\", \"ptb\"]\n",
    "\n",
    "for ft in file_types:\n",
    "    dfs = []\n",
    "    # 3) 각 result_* 폴더 안의 {ft}_counts.csv 파일들 읽어오기\n",
    "    pattern = os.path.join(base_dir, \"result_*\", f\"{ft}_counts.csv\")\n",
    "    for csv_path in glob.glob(pattern):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # 4) sample 컬럼에 폴더 이름 추가\n",
    "        sample = os.path.basename(os.path.dirname(csv_path))\n",
    "        df.insert(0, \"sample\", sample)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"[{ft}] No files found under pattern: {pattern}\")\n",
    "        continue\n",
    "\n",
    "    # 5) 모든 df 합치기\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 6) 자동으로 첫 3개 컬럼 인식: index, pivot_col, value\n",
    "    index_col = combined.columns[0]   # sample\n",
    "    pivot_col = combined.columns[1]   # pt_bin or mass_bin 등\n",
    "    value_col = combined.columns[2]   # count\n",
    "\n",
    "    # 7) pivot & 결측치는 0으로\n",
    "    pivot = combined.pivot(\n",
    "        index=index_col,\n",
    "        columns=pivot_col,\n",
    "        values=value_col\n",
    "    ).fillna(0)\n",
    "\n",
    "    # 8) 결과 파일명 & 저장\n",
    "    outname = os.path.join(\n",
    "        base_dir,\n",
    "        f\"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/bkg_{ft}_counts_pivot.csv\"\n",
    "    )\n",
    "    pivot.to_csv(outname, index=True)\n",
    "    print(f\"[{ft}] Saved pivot CSV: {outname} ({pivot.shape[0]} samples × {pivot.shape[1]} bins)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256fafc",
   "metadata": {},
   "source": [
    "#### sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdbd14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wrmass] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_wrmass_counts_pivot.csv (8 samples × 10 bins)\n",
      "[mll] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_mll_counts_pivot.csv (8 samples × 13 bins)\n",
      "[ptmu1] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_ptmu1_counts_pivot.csv (8 samples × 12 bins)\n",
      "[ptmu2] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_ptmu2_counts_pivot.csv (8 samples × 12 bins)\n",
      "[ptt] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_ptt_counts_pivot.csv (8 samples × 14 bins)\n",
      "[ptb] Saved pivot CSV: /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_ptb_counts_pivot.csv (8 samples × 14 bins)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1) 베이스 디렉토리 설정\n",
    "base_dir = \"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig/condorfile\"\n",
    "\n",
    "# 2) 처리할 카운트 파일 종류 리스트\n",
    "file_types = [\"wrmass\", \"mll\", \"ptmu1\", \"ptmu2\", \"ptt\", \"ptb\"]\n",
    "\n",
    "for ft in file_types:\n",
    "    dfs = []\n",
    "    # 3) 각 result_* 폴더 안의 {ft}_counts.csv 파일들 읽어오기\n",
    "    pattern = os.path.join(base_dir, \"result_*\", f\"{ft}_counts.csv\")\n",
    "    for csv_path in glob.glob(pattern):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # 4) sample 컬럼에 폴더 이름 추가\n",
    "        sample = os.path.basename(os.path.dirname(csv_path))\n",
    "        df.insert(0, \"sample\", sample)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"[{ft}] No files found under pattern: {pattern}\")\n",
    "        continue\n",
    "\n",
    "    # 5) 모든 df 합치기\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 6) 자동으로 첫 3개 컬럼 인식: index, pivot_col, value\n",
    "    index_col = combined.columns[0]   # sample\n",
    "    pivot_col = combined.columns[1]   # pt_bin or mass_bin 등\n",
    "    value_col = combined.columns[2]   # count\n",
    "\n",
    "    # 7) pivot & 결측치는 0으로\n",
    "    pivot = combined.pivot(\n",
    "        index=index_col,\n",
    "        columns=pivot_col,\n",
    "        values=value_col\n",
    "    ).fillna(0)\n",
    "\n",
    "    # 8) 결과 파일명 & 저장\n",
    "    outname = os.path.join(\n",
    "        base_dir,\n",
    "        f\"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/sig_{ft}_counts_pivot.csv\"\n",
    "    )\n",
    "    pivot.to_csv(outname, index=True)\n",
    "    print(f\"[{ft}] Saved pivot CSV: {outname} ({pivot.shape[0]} samples × {pivot.shape[1]} bins)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d27161",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392b75c",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a53660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-1000_MN-700_13p6TeV_wrmass_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-1300_13p6TeV_wrmass_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2100_13p6TeV_wrmass_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2900_13p6TeV (wrmass): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-2500_13p6TeV_wrmass_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-3300_13p6TeV (wrmass): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4100_13p6TeV (wrmass): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4900_13p6TeV (wrmass): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-1000_MN-700_13p6TeV_mll_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-1300_13p6TeV_mll_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2100_13p6TeV_mll_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2900_13p6TeV (mll): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-2500_13p6TeV_mll_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-3300_13p6TeV (mll): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4100_13p6TeV (mll): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4900_13p6TeV (mll): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-1000_MN-700_13p6TeV_ptmu1_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-1300_13p6TeV_ptmu1_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2100_13p6TeV_ptmu1_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2900_13p6TeV (ptmu1): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-2500_13p6TeV (ptmu1): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-3300_13p6TeV (ptmu1): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4100_13p6TeV (ptmu1): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4900_13p6TeV (ptmu1): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-1000_MN-700_13p6TeV_ptmu2_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-1300_13p6TeV_ptmu2_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2100_13p6TeV_ptmu2_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2900_13p6TeV (ptmu2): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-2500_13p6TeV (ptmu2): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-3300_13p6TeV (ptmu2): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4100_13p6TeV (ptmu2): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4900_13p6TeV (ptmu2): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-1000_MN-700_13p6TeV_ptt_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-1300_13p6TeV_ptt_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2100_13p6TeV_ptt_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2900_13p6TeV (ptt): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-2500_13p6TeV (ptt): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-3300_13p6TeV (ptt): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4100_13p6TeV (ptt): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4900_13p6TeV (ptt): all < 1\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-1000_MN-700_13p6TeV_ptb_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-1300_13p6TeV_ptb_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2100_13p6TeV_ptb_stacked.png\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-3000_MN-2900_13p6TeV (ptb): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-2500_13p6TeV (ptb): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-3300_13p6TeV (ptb): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4100_13p6TeV (ptb): all < 1\n",
      "✖ Skipped result_WRtoNMutoMuMuTB-HadTop_MWR-5000_MN-4900_13p6TeV (ptb): all < 1\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "out_dir  = base_dir\n",
    "\n",
    "def parse_bin_label(lbl):\n",
    "    parts = re.split(r\"[–\\-]\", lbl)\n",
    "    low  = float(parts[0])\n",
    "    high = np.inf if parts[1].lower() in (\"inf\",\"infinity\") else float(parts[1])\n",
    "    return low, high, lbl\n",
    "\n",
    "def extract_MWR(sample):\n",
    "    m = re.search(r\"MWR-(\\d+)\", sample)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "for sig_path in glob.glob(os.path.join(base_dir, \"sig_*_counts_pivot.csv\")):\n",
    "    var = os.path.basename(sig_path).split(\"_\")[1]\n",
    "    df_sig = pd.read_csv(sig_path, index_col=\"sample\")\n",
    "    bkg_path = os.path.join(base_dir, f\"bkg_{var}_counts_pivot.csv\")\n",
    "    if not os.path.exists(bkg_path): continue\n",
    "    df_bkg = pd.read_csv(bkg_path, index_col=\"sample\")\n",
    "\n",
    "    # bin parsing & sort\n",
    "    bin_infos = sorted((parse_bin_label(lbl) for lbl in df_sig.columns),\n",
    "                       key=lambda x: x[0])\n",
    "    lows    = np.array([b[0] for b in bin_infos])\n",
    "    highs   = np.array([b[1] for b in bin_infos])\n",
    "    labels  = [b[2] for b in bin_infos]\n",
    "    widths  = highs - lows\n",
    "    widths[np.isinf(widths)] = widths[~np.isinf(widths)][-1]\n",
    "    centers = (lows + highs) / 2\n",
    "\n",
    "    # background stacks\n",
    "    totals = df_bkg.sum(axis=1).sort_values(ascending=False)\n",
    "    top2, others = totals.index[:2].tolist(), totals.index[2:].tolist()\n",
    "    bkg_other = df_bkg.loc[others].sum().reindex(labels).values\n",
    "    bkg_top2  = df_bkg.loc[top2[1]].reindex(labels).values\n",
    "    bkg_top1  = df_bkg.loc[top2[0]].reindex(labels).values\n",
    "\n",
    "    finite_highs = highs[np.isfinite(highs)]\n",
    "    max_high     = finite_highs.max() if len(finite_highs) else highs[-1]\n",
    "\n",
    "    for sample, sig_row in df_sig.iterrows():\n",
    "        sig_vals   = sig_row.reindex(labels).values\n",
    "        base_stack = bkg_other + bkg_top2 + bkg_top1\n",
    "        y_sig      = base_stack + sig_vals\n",
    "\n",
    "        if np.max(y_sig) < 1.0:\n",
    "            print(f\"✖ Skipped {sample} ({var}): all < 1\")\n",
    "            continue\n",
    "\n",
    "        # x-axis end\n",
    "        if var == \"wrmass\":\n",
    "            MWR = extract_MWR(sample) or max_high\n",
    "            extended_end = MWR + 3000\n",
    "            ext_low   = lows[-1]\n",
    "            ext_width = extended_end - ext_low\n",
    "            last_val  = y_sig[-1]\n",
    "        else:\n",
    "            last_low  = lows[-1]\n",
    "            last_high = highs[-1]\n",
    "            extended_end = last_low + 1000 if np.isinf(last_high) else last_high\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        bottom = np.zeros_like(lows)\n",
    "\n",
    "        # plot background\n",
    "        ax.bar(lows, bkg_other, widths, bottom=0, label=\"other bkg\", align=\"edge\")\n",
    "        bottom += bkg_other\n",
    "        ax.bar(lows, bkg_top2, widths, bottom=bottom, label=top2[1], align=\"edge\")\n",
    "        bottom += bkg_top2\n",
    "        ax.bar(lows, bkg_top1, widths, bottom=bottom, label=top2[0], align=\"edge\")\n",
    "\n",
    "        # extension for wrmass\n",
    "        if var == \"wrmass\":\n",
    "            ax.bar(ext_low, bkg_other[-1], ext_width, bottom=0,\n",
    "                   color=ax.containers[0].patches[0].get_facecolor(), align=\"edge\")\n",
    "            ax.bar(ext_low, bkg_top2[-1], ext_width, bottom=bkg_other[-1],\n",
    "                   color=ax.containers[1].patches[0].get_facecolor(), align=\"edge\")\n",
    "            ax.bar(ext_low, bkg_top1[-1], ext_width, bottom=bkg_other[-1]+bkg_top2[-1],\n",
    "                   color=ax.containers[2].patches[0].get_facecolor(), align=\"edge\")\n",
    "            # extend y_sig\n",
    "            lows_ext = np.append(lows, ext_low)\n",
    "            highs_ext= np.append(highs, extended_end)\n",
    "            y_sig_ext= np.append(y_sig, last_val)\n",
    "        else:\n",
    "            lows_ext  = lows.copy()\n",
    "            highs_ext = highs.copy()\n",
    "            y_sig_ext = y_sig.copy()\n",
    "\n",
    "        # overlay signal as dashed-rectangle\n",
    "        for low, high, top in zip(lows_ext, highs_ext, y_sig_ext):\n",
    "            rect = Rectangle(\n",
    "                (low, 0),               # x,y of lower-left\n",
    "                high - low,             # width\n",
    "                top,                    # height\n",
    "                fill=False,\n",
    "                linestyle='--',\n",
    "                linewidth=1.5,\n",
    "                edgecolor='black',\n",
    "                zorder=5\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # final formatting\n",
    "        ax.set_xlim(0, extended_end)\n",
    "        ax.set_xticks(np.arange(0, extended_end+1, 500))\n",
    "        ax.set_xlabel(var)\n",
    "        ax.set_ylabel(\"Weighted count\")\n",
    "        ax.set_title(f\"Stacked {var} for {sample}\")\n",
    "        ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "        plt.tight_layout()\n",
    "        # 6) y축 로그 스케일\n",
    "        ax.set_yscale('log')\n",
    "        outpath = os.path.join(out_dir, f\"{sample}_{var}_stacked.png\")\n",
    "        fig.savefig(outpath)\n",
    "        plt.close(fig)\n",
    "        print(f\"✔ Saved {outpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fff84c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/wrmass_all_signals_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/mll_all_signals_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/ptmu1_all_signals_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/ptmu2_all_signals_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/ptt_all_signals_stacked.png\n",
      "✔ Saved /data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0/ptb_all_signals_stacked.png\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = \"/data6/Users/achihwan/LRSM_tb_channel/SAMPLEPRODUCTION/using_matrix/condor/1AK4_1AK8/result/btag_PN_tight_hlt_30/WR_cut_0/top_0.9_120_250/mll0pt0eta0\"\n",
    "out_dir  = base_dir\n",
    "\n",
    "def parse_bin_label(lbl):\n",
    "    parts = re.split(r\"[–\\-]\", lbl)\n",
    "    low  = float(parts[0])\n",
    "    high = np.inf if parts[1].lower() in (\"inf\",\"infinity\") else float(parts[1])\n",
    "    return low, high, lbl\n",
    "\n",
    "def extract_MWR(sample):\n",
    "    m = re.search(r\"MWR-(\\d+)\", sample)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "for sig_path in glob.glob(os.path.join(base_dir, \"sig_*_counts_pivot.csv\")):\n",
    "    var = os.path.basename(sig_path).split(\"_\")[1]\n",
    "    df_sig = pd.read_csv(sig_path, index_col=\"sample\")\n",
    "    bkg_file = os.path.join(base_dir, f\"bkg_{var}_counts_pivot.csv\")\n",
    "    if not os.path.exists(bkg_file): continue\n",
    "    df_bkg = pd.read_csv(bkg_file, index_col=\"sample\")\n",
    "\n",
    "    # --- bin 세팅 ---\n",
    "    bin_infos = sorted((parse_bin_label(lbl) for lbl in df_sig.columns),\n",
    "                       key=lambda x: x[0])\n",
    "    lows   = np.array([b[0] for b in bin_infos])\n",
    "    highs  = np.array([b[1] for b in bin_infos])\n",
    "    widths = highs - lows\n",
    "    widths[np.isinf(widths)] = widths[~np.isinf(widths)][-1]\n",
    "    labels = [b[2] for b in bin_infos]\n",
    "\n",
    "    # background 스택 값\n",
    "    totals = df_bkg.sum(axis=1).sort_values(ascending=False)\n",
    "    top2, others = totals.index[:2].tolist(), totals.index[2:].tolist()\n",
    "    bkg_other = df_bkg.loc[others].sum().reindex(labels).values\n",
    "    bkg_top2  = df_bkg.loc[top2[1]].reindex(labels).values\n",
    "    bkg_top1  = df_bkg.loc[top2[0]].reindex(labels).values\n",
    "\n",
    "    # x축 끝점 계산 (모든 시그널에서 최대값 찾기)\n",
    "    finite_highs = highs[np.isfinite(highs)]\n",
    "    max_high = finite_highs.max() if len(finite_highs) else highs[-1]\n",
    "    \n",
    "    if var == \"wrmass\":\n",
    "        # 모든 시그널의 MWR 값을 구해서 최대값 찾기\n",
    "        all_MWRs = [extract_MWR(s) for s in df_sig.index if extract_MWR(s) is not None]\n",
    "        if all_MWRs:\n",
    "            max_MWR = max(all_MWRs)\n",
    "            x_end = max_MWR + 3000\n",
    "        else:\n",
    "            x_end = max_high + 3000\n",
    "    else:\n",
    "        last_high = highs[-1]\n",
    "        x_end = lows[-1] + 1000 if np.isinf(last_high) else last_high\n",
    "\n",
    "    xticks = np.arange(0, x_end+1, 500)\n",
    "\n",
    "    # --- background 스택 플롯 ---\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    bottom = np.zeros_like(lows)\n",
    "    ax.bar(lows, bkg_other, widths, bottom=bottom, label=\"other bkg\", align=\"edge\")\n",
    "    bottom += bkg_other\n",
    "    ax.bar(lows, bkg_top2, widths,  bottom=bottom, label=top2[1], align=\"edge\")\n",
    "    bottom += bkg_top2\n",
    "    ax.bar(lows, bkg_top1, widths,  bottom=bottom, label=top2[0], align=\"edge\")\n",
    "\n",
    "    # --- 모든 시그널 오버레이 (가로선만) ---\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    for i, (sample, sig_row) in enumerate(df_sig.iterrows()):\n",
    "        # signal 높이\n",
    "        sig_vals   = sig_row.reindex(labels).values\n",
    "        heights    = bottom + sig_vals\n",
    "\n",
    "        color = cmap(i % cmap.N)\n",
    "        \n",
    "        # 각 bin에 대해 가로선만 그리기\n",
    "        for j, (low, high, height) in enumerate(zip(lows, highs, heights)):\n",
    "            if np.isinf(high):\n",
    "                # 마지막 bin이 무한대인 경우, 모든 시그널에 대해 동일한 x_end 사용\n",
    "                high = x_end\n",
    "            \n",
    "            # 가로선만 그리기 (세로선 없음)\n",
    "            if j == 0:  # 첫 번째 선에만 label 추가\n",
    "                ax.plot([low, high], [height, height], \n",
    "                       color=color, linewidth=2, label=sample)\n",
    "            else:\n",
    "                ax.plot([low, high], [height, height], \n",
    "                       color=color, linewidth=2)\n",
    "\n",
    "    # --- 축/레이블 ---\n",
    "    ax.set_xlim(0, x_end)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([f\"{int(x)}\" for x in xticks], rotation=45)\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel(\"Weighted count\")\n",
    "    ax.set_title(f\"Stacked backgrounds + all signals ({var})\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # y축 로그 스케일 및 y축 상한 조정\n",
    "    ax.set_yscale('log')\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.set_ylim(y_min, y_max * 100)  # log 스케일에서 약 2단계(10^2) 상향\n",
    "    \n",
    "    # --- 저장 ---\n",
    "    outpath = os.path.join(out_dir, f\"{var}_all_signals_stacked.png\")\n",
    "    fig.savefig(outpath)\n",
    "    plt.close(fig)\n",
    "    print(f\"✔ Saved {outpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003260f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
